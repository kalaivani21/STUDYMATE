# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PYk9MbnkoPBR540j6qFbpF8nDuP03gXF
"""

!pip install streamlit colabcode PyMuPDF sentence-transformers faiss-cpu transformers

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import fitz  # PyMuPDF
# import faiss
# import numpy as np
# from sentence_transformers import SentenceTransformer
# from transformers import pipeline
# 
# # ------------------ Embedding Model ------------------
# embedder = SentenceTransformer("all-MiniLM-L6-v2")
# 
# # ------------------ Local QA Model ------------------
# qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-small")
# 
# # ------------------ PDF Text Extraction ------------------
# def extract_text_from_pdfs(uploaded_files):
#     texts = []
#     for pdf_file in uploaded_files:
#         doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
#         for page in doc:
#             texts.append(page.get_text())
#     return texts
# 
# # ------------------ Chunking ------------------
# def chunk_text(texts, chunk_size=500):
#     chunks = []
#     for text in texts:
#         words = text.split()
#         for i in range(0, len(words), chunk_size):
#             chunk = " ".join(words[i:i+chunk_size])
#             chunks.append(chunk)
#     return chunks
# 
# # ------------------ Build FAISS Index ------------------
# def build_faiss(chunks):
#     embeddings = embedder.encode(chunks)
#     dim = embeddings.shape[1]
#     index = faiss.IndexFlatL2(dim)
#     index.add(np.array(embeddings))
#     return index, chunks
# 
# # ------------------ Semantic Search ------------------
# def search(query, index, chunks, top_k=3):
#     query_emb = embedder.encode([query])
#     distances, indices = index.search(np.array(query_emb), top_k)
#     results = [chunks[i] for i in indices[0]]
#     return results
# 
# # ------------------ Answer Generation ------------------
# def generate_answer(query, context_chunks):
#     context_text = "\n".join(context_chunks)
#     prompt = f"Answer the question based on the following context:\n{context_text}\n\nQuestion: {query}\nAnswer:"
#     response = qa_pipeline(prompt, max_length=200, do_sample=False)
#     return response[0]['generated_text']
# 
# # ------------------ Streamlit UI ------------------
# st.title("ðŸ“˜ StudyMate â€“ AI Academic Assistant (No API Key)")
# st.write("Upload your PDFs and ask questions in natural language.")
# 
# uploaded_files = st.file_uploader("Upload PDF(s)", type="pdf", accept_multiple_files=True)
# 
# if uploaded_files:
#     st.success("PDF(s) uploaded successfully!")
# 
#     if st.button("Build Knowledge Base"):
#         texts = extract_text_from_pdfs(uploaded_files)
#         chunks = chunk_text(texts)
#         index, all_chunks = build_faiss(chunks)
#         st.session_state["index"] = index
#         st.session_state["chunks"] = all_chunks
#         st.success("Knowledge base built! You can now ask questions.")
# 
# if "index" in st.session_state:
#     query = st.text_input("Ask a question about your study material:")
#     if query:
#         context = search(query, st.session_state["index"], st.session_state["chunks"])
#         answer = generate_answer(query, context)
#         st.subheader("Answer:")
#         st.write(answer)
#         st.subheader("Context:")
#         st.write("\n\n".join(context))
#

# Install cloudflared (only once per session)
!pip install -q streamlit
!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb
!dpkg -i cloudflared-linux-amd64.deb

# Run Streamlit + Cloudflare Tunnel
!streamlit run app.py --server.port 8501 & cloudflared tunnel --url http://localhost:8501

